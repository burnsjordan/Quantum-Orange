{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# code from Jordan\n",
    "Identity = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "def tuple_array(arr):\n",
    "    temp = []\n",
    "    temp.append(np.size(arr, 0))\n",
    "    for i in range(np.size(arr, 0)):\n",
    "        for j in range(np.size(arr, 1)):\n",
    "            temp.append(arr[i][j])\n",
    "    return tuple(temp)\n",
    "\n",
    "\n",
    "def untuple_array(arr):\n",
    "    temp = []\n",
    "    N = arr[0]\n",
    "    for i in range(N):\n",
    "        temp2 = []\n",
    "        for j in range(N):\n",
    "            temp2.append(arr[N*i+j+1])\n",
    "        temp.append(temp2)\n",
    "    return np.array(temp)\n",
    "\n",
    "# Small set of quantum gates\n",
    "small_dict = {\n",
    "    'Identity': np.array([[1, 0], [0, 1]]),\n",
    "    'X': np.array([[0, 1], [1, 0]]),\n",
    "    'CNOT': np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]),\n",
    "    'TONC': np.array([[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]),\n",
    "    'Hadamard': (1/(2**(1/2)))*np.array([[1, 1], [1, -1]]),\n",
    "    'Pi8': np.array([[1, 0], [0, np.exp(1j*np.pi/4)]])\n",
    "}\n",
    "\n",
    "\n",
    "# Return all possible gates from a given list of size N\n",
    "def get_gates(N, list_size):\n",
    "    list = {}\n",
    "    if(N == 1):\n",
    "        count = 0\n",
    "        list[1] = small_dict['Identity']\n",
    "        list[2] = small_dict['X']\n",
    "        list[3] = small_dict['Hadamard']\n",
    "        list[4] = small_dict['Pi8']\n",
    "    else:\n",
    "        count = 0\n",
    "        for i in small_dict:\n",
    "            for j in range(1, N+1):\n",
    "                marker = 0\n",
    "                if(j == 1):\n",
    "                    temp_matrix = small_dict[i]\n",
    "                    if(i == 'CNOT' or i == 'TONC'):\n",
    "                        marker = 1\n",
    "                else:\n",
    "                    temp_matrix = Identity\n",
    "                identity = Identity\n",
    "                k = 2 + marker\n",
    "                double = False\n",
    "                while(k < N+1):\n",
    "                    if(k == j):\n",
    "                        if(k == N and (i == 'CNOT' or i == 'TONC')):\n",
    "                            double = True\n",
    "                        else:\n",
    "                            temp_matrix = np.kron(temp_matrix, small_dict[i])\n",
    "                        if(i == 'CNOT' or i == 'TONC'):\n",
    "                            k += 1\n",
    "                    else:\n",
    "                        temp_matrix = np.kron(temp_matrix, identity)\n",
    "                    k += 1\n",
    "                if(j!=1 and i=='Identity'):\n",
    "                    double = True\n",
    "                if(not double):\n",
    "                    list[count] = temp_matrix\n",
    "                    count += 1\n",
    "    return(list)\n",
    "\n",
    "\n",
    "# Produce training data for a 4x4 matrix\n",
    "# Returns an array which has the array of inputs as the first element\n",
    "# and the array of outputs as the second element\n",
    "def get_training_data(N):\n",
    "    # Setup to build training data\n",
    "    inputs = []\n",
    "    last_inputs = []\n",
    "    outputs = []\n",
    "    gates_list = get_gates(N, \"small\")\n",
    "    identity_hit_count = 0\n",
    "    inverse_hit_count = 0\n",
    "    last_input = 0\n",
    "\n",
    "    # Build training data\n",
    "    # i*j is the number of data points\n",
    "    for i in range(10000):\n",
    "        input_matrix = np.kron(Identity, Identity)\n",
    "        last_input = input_matrix\n",
    "        for j in range(2):\n",
    "            identity = np.identity(2**N)\n",
    "            dont_add = True\n",
    "            while(dont_add):\n",
    "                dont_add = False\n",
    "                temp = gates_list[random.choice(list(gates_list.keys()))]\n",
    "                if(tuple_array(temp) == tuple_array(identity)):\n",
    "                    dont_add = True\n",
    "                    identity_hit_count += 1\n",
    "                if(outputs):\n",
    "                    if(tuple_array(np.matmul(outputs[-1], temp)) == tuple_array(identity)):\n",
    "                        dont_add = True\n",
    "                        inverse_hit_count += 1\n",
    "                if(not dont_add):\n",
    "                    input_matrix = np.dot(input_matrix, temp)\n",
    "                    inputs.append(input_matrix)\n",
    "                    last_inputs.append(last_input)\n",
    "                    outputs.append(temp)\n",
    "                    last_input = input_matrix\n",
    "\n",
    "    # Print the first 10 inputs and outputs if you want\n",
    "    # for i in range(10):\n",
    "    #     print(str(i+1) + ':')\n",
    "    #     print(inputs[i])\n",
    "    #     print(outputs[i])\n",
    "\n",
    "    # Print number of times the identity or an inverse is chosen and ignored\n",
    "    # print(\"identity_hit_count: \" + str(identity_hit_count))\n",
    "    # print(\"inverse_hit_count: \" + str(inverse_hit_count))\n",
    "\n",
    "    return [inputs, outputs, last_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "inputs length: 20000\noutputs length: 20000\nlast inputs length:  20000\n"
    }
   ],
   "source": [
    "# get the list of inputs and outputs\n",
    "inputs, outputs, last_inputs = get_training_data(2)\n",
    "print('inputs length:', len(inputs))\n",
    "print('outputs length:', len(outputs))\n",
    "print('last inputs length: ', len(last_inputs))\n",
    "# take a look at some of the input output pairs to get a better idea of the data\n",
    "# it looks like there are multiple matrces that are the same that have different outputs...\n",
    "# for i in range(5):\n",
    "#         print(str(i+1) + ':')\n",
    "#         print(inputs[i])\n",
    "#         print(outputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(20000, 4, 4, 4)"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split up each input matrix into a real part matrix and an imaginary part matrix.\n",
    "# inputs[12].real\n",
    "\n",
    "# inputs_real = np.array([])\n",
    "# inputs_imag = np.array([])\n",
    "# for i in inputs:\n",
    "#     np.append(inputs_real, i.real)\n",
    "#     np.append(inputs_imag, i.imag)\n",
    "\n",
    "inputs_real = []\n",
    "inputs_imag = []\n",
    "last_inputs_real = []\n",
    "last_inputs_imag = []\n",
    "for i in inputs:\n",
    "    inputs_real.append(i.real)\n",
    "    inputs_imag.append(i.imag)\n",
    "for j in last_inputs:\n",
    "    last_inputs_real.append(j.real)\n",
    "    last_inputs_imag.append(j.imag)\n",
    "inputs_r = np.asarray(inputs_real)\n",
    "inputs_i = np.asarray(inputs_imag)\n",
    "last_inputs_r = np.asarray(last_inputs_real)\n",
    "last_inputs_i = np.asarray(last_inputs_imag)\n",
    "inputs_ = np.stack([inputs_r, inputs_i, last_inputs_r, last_inputs_i], 3)\n",
    "inputs_.shape # 10000, 4, 4, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(20000, 4, 4)"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next get outputs into np array (10000, 1)\n",
    "outputs_ = np.asarray(outputs)\n",
    "outputs_.shape # 10000, 4, 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the possible outputs from Jordan\n",
    "one = np.array([[0, 0, 1, 0],\n",
    "                [0, 0, 0, 1],\n",
    "                [1, 0, 0, 0],\n",
    "                [0, 1, 0, 0]])\n",
    "\n",
    "two = np.array([[0, 1, 0, 0],\n",
    "                [1, 0, 0, 0],\n",
    "                [0, 0, 0, 1],\n",
    "                [0, 0, 1, 0]])\n",
    "\n",
    "three = np.array([[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 0, 1],\n",
    "                  [0, 0, 1, 0]])\n",
    "four = np.array([[ 0.70710678,  0.,          0.70710678,  0.        ],\n",
    "                 [ 0.,          0.70710678,  0.,          0.70710678],\n",
    "                 [ 0.70710678,  0.,         -0.70710678, -0.,        ],\n",
    "                 [ 0.,          0.70710678, -0.,         -0.70710678]])\n",
    "\n",
    "five = np.array([[ 0.70710678,  0.70710678,  0.,          0.        ],\n",
    " [ 0.70710678, -0.70710678,  0.,         -0.        ],\n",
    " [ 0.,          0.,          0.70710678,  0.70710678],\n",
    " [ 0.,         -0.,          0.70710678, -0.70710678]])\n",
    "\n",
    "six = np.array([[1.        +0.j,         0.        +0.j,         0.        +0.j,\n",
    "  0.        +0.j        ],\n",
    " [0.        +0.j,         1.        +0.j,         0.        +0.j,\n",
    "  0.        +0.j        ],\n",
    " [0.        +0.j,         0.        +0.j,         0.70710678+0.70710678j,\n",
    "  0.        +0.j        ],\n",
    " [0.        +0.j,         0.        +0.j,         0.        +0.j,\n",
    "  0.70710678+0.70710678j]])\n",
    "\n",
    "seven = np.array([[1.        +0.j,         0.        +0.j,         0.        +0.j,\n",
    "  0.        +0.j        ],\n",
    " [0.        +0.j,         0.70710678+0.70710678j, 0.        +0.j,\n",
    "  0.        +0.j        ],\n",
    " [0.        +0.j,         0.        +0.j,         1.        +0.j,\n",
    "  0.        +0.j        ],\n",
    " [0.        +0.j,         0.        +0.j,         0.        +0.j,\n",
    "  0.70710678+0.70710678j]])\n",
    "\n",
    "eight = np.array([[1, 0, 0, 0],\n",
    " [0, 1, 0, 0],\n",
    " [0, 0, 1, 0],\n",
    " [0, 0, 0, 1]])\n",
    "\n",
    "nine = np.array([[0, 1, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 1, 0],\n",
    " [0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20000\n0\n"
    }
   ],
   "source": [
    "# map the output matricies to classes 1, 2, 3.. \n",
    "output_classes = []\n",
    "count = 0\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    if (np.allclose(one, outputs[i])):\n",
    "        output_classes.append(1)\n",
    "    elif (np.allclose(two, outputs[i])):\n",
    "        output_classes.append(2)\n",
    "    elif (np.allclose(three, outputs[i])):\n",
    "        output_classes.append(3)\n",
    "    elif (np.allclose(four, outputs[i])):\n",
    "        output_classes.append(4)\n",
    "    elif (np.allclose(five, outputs[i])):\n",
    "        output_classes.append(5)\n",
    "    elif (np.allclose(six, outputs[i])):\n",
    "        output_classes.append(6)\n",
    "    elif (np.allclose(seven, outputs[i])):\n",
    "        output_classes.append(7)\n",
    "    elif (np.allclose(eight, outputs[i])):\n",
    "        output_classes.append(8)\n",
    "    elif (np.allclose(nine, outputs[i])):\n",
    "        output_classes.append(9)\n",
    "    else: \n",
    "#         print('error')\n",
    "        count += 1\n",
    "        output_classes.append(-1)\n",
    "\n",
    "print(len(output_classes))\n",
    "print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(20000,) (20000, 4, 4, 4)\n"
    }
   ],
   "source": [
    "# this way isnt working for all outputs. must get all the unique arrays in the \n",
    "# outputs list, for now just use the ones that work. outputs (1,2,3,8,9)\n",
    "# get indecies where output_classes = -1 and remove those indecies from inputs and ouptuclasses\n",
    "outputss = np.array(output_classes)\n",
    "indecies = np.where(outputss == -1)\n",
    "outputss = np.delete(outputss, indecies)\n",
    "inputs_ = np.delete(inputs_, indecies, axis=0)\n",
    "print(outputss.shape, inputs_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputss[outputss == 8] = 0\n",
    "# outputss[outputss == 9] = 4\n",
    "\n",
    "# inputs_[[0,1]]\n",
    "# inputs_.shape[0]\n",
    "# np.unique(outputss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test and normalize the inputs\n",
    "index = np.arange(inputs_.shape[0])\n",
    "num_training = int(inputs_.shape[0]*.8)\n",
    "train_inputs, train_outputs = inputs_[:num_training], outputss[:num_training]\n",
    "test_inputs, test_outputs = inputs_[num_training:], outputss[num_training:]\n",
    "\n",
    "# Normalize inputs values between 1 and 0\n",
    "# lets not for now. \n",
    "\n",
    "# num_training\n",
    "# test_inputs.shape\n",
    "# train_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_15 (Conv2D)           (None, 4, 4, 32)          96        \n=================================================================\nTotal params: 96\nTrainable params: 96\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# set up the network architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (1, 1), activation='relu', input_shape=(4, 4, 4)))\n",
    "# model.add(layers.Conv2D(32, (1, 1), activation='relu', input_shape=(4, 4, 4)))\n",
    "# model.add(layers.Conv2D(32, (1, 1), activation='relu', input_shape=(4, 4, 4)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "# model.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "# model.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# num of output classes\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_15_input to have shape (4, 4, 2) but got array with shape (4, 4, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-4d32d26bb8c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m history = model.fit(train_inputs, train_outputs, epochs=10, \n\u001b[1;32m----> 7\u001b[1;33m                     validation_data=(test_inputs, test_outputs))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2472\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    572\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    575\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_15_input to have shape (4, 4, 2) but got array with shape (4, 4, 4)"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_inputs, train_outputs, epochs=10, \n",
    "                    validation_data=(test_inputs, test_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_inputs,  test_outputs, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('low_depth_nn_2_mi.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(train_inputs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}